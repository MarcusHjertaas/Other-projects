{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Kochmar mentions several steps required in a typical NLP pipeline, one of them being *Split into words*. Why is this step necessary? Why can we not just feed the text as it is into a model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to split into words as this allows for individual analysis of the words in a scentence, this can be more computationaly efficient and allow for methods like paralell processing. Seperate words are also easier to utilise in feature extraction and semantic analysis so the computer can understand which are the important words and which only provide gramatical meaning. It is perhaps also easier for it to construct context and se words in different contexts establishing patterns of recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Simply splitting on \"words\" (i.e. whitespace) is rarely enough. Consider the sentence below (\"That U.S.A. poster-print costs $12.40...\") and name some problems that arise from splitting on whitespace.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problems with using whitespace as a divider arises when words use special characters, abbriviations or numeric values. In general a simple fault is the dot that seperates scenteces. In a case where you only use whitespace the dot would be included as a part of the last token in the scentence likely causing the word to be unrecougnizable. In this spesific example the main issue occurs with the pricing that is tokenized into $12.40... which would not be the same as a $12.4 despite them having the same interpreted meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40...']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"That U.S.A. poster-print costs $12.40...\"\n",
    "sentence.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you wish, experiment with implementing different rules for tokenization. You will see that the \"ruleset\" quickly grows if you want to account for all types of edge cases...\n",
    "sentence = \"That U.S.A. poster-print costs $12.40...\"\n",
    "\n",
    "def your_rulebased_tokenizer(sentence):\n",
    "    tokens = []\n",
    "    return tokens\n",
    "\n",
    "your_rulebased_tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marcu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the Punkt tokenizer models\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has several tokenizers implemented, such as a specific one for Twitter data. Below, indicated by the `TODO`-tag, you should find and import various tokenizers and add them to the list of tokenizers:\n",
    "\n",
    "`tokenizers = [tokenizer1, tokenizer2, ..., tokenizerN]`\n",
    "\n",
    "Tokenize the sentence with at least three different tokenizers supplied by NLTK and comment on your findings. You will find the documentation for NLTK's tokenizers [here](https://www.nltk.org/_modules/nltk/tokenize.html) useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyWhitespaceTokenizer (5 tokens)\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40...']\n",
      "\n",
      "SpaceDot_Tokenizer (16 tokens)\n",
      "['That', 'U', '.', 'S', '.', 'A', '.', 'poster', '-', 'print', 'costs', '$', '12', '.', '40', '...']\n",
      "\n",
      "Sentence_Tokenizer (1 tokens)\n",
      "['That U.S.A. poster-print costs $12.40...']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "# this is the base class of tokenizers in nltk\n",
    "from nltk.tokenize.api import TokenizerI\n",
    "from nltk.tokenize import wordpunct_tokenize, sent_tokenize \n",
    "\n",
    "\n",
    "# this is just a simple example of how a tokenizer can be implemented\n",
    "class MyWhitespaceTokenizer(TokenizerI):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        return text.split()\n",
    "    \n",
    "class SpaceDot_Tokenizer(TokenizerI):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "       return wordpunct_tokenize(text)\n",
    "\n",
    "\n",
    "class Sentence_Tokenizer(TokenizerI):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        return sent_tokenize(text)\n",
    "\n",
    "\n",
    "sentence = \"That U.S.A. poster-print costs $12.40...\"\n",
    "\n",
    "# ************************************************************\n",
    "# TODO: import and add the tokenizers you want to try out here\n",
    "# ************************************************************\n",
    "tokenizers = [\n",
    "    MyWhitespaceTokenizer(), \n",
    "    SpaceDot_Tokenizer(),\n",
    "    Sentence_Tokenizer()\n",
    "]\n",
    "\n",
    "\n",
    "# Leave this function as-is\n",
    "def tokenize(tokenizers: List[TokenizerI], sentence: str) -> None:\n",
    "    for tokenizer in tokenizers:\n",
    "        assert isinstance(tokenizer, TokenizerI)\n",
    "        tokenized = tokenizer.tokenize(sentence)\n",
    "        print(f\"{tokenizer.__class__.__name__} ({len(tokenized)} tokens)\\n{tokenized}\\n\")\n",
    "\n",
    "tokenize(tokenizers, sentence)\n",
    "\n",
    "# This was my first attempt, surprisingly it worked,\n",
    "# but i used the same framework of classes and functions not realising this was not the correct way to approch this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyWhitespaceTokenizer (5 tokens)\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40...']\n",
      "\n",
      "NLTKWordTokenizer (7 tokens)\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$', '12.40', '...']\n",
      "\n",
      "MWETokenizer (40 tokens)\n",
      "['T', 'h', 'a', 't', ' ', 'U', '.', 'S', '.', 'A', '.', ' ', 'p', 'o', 's', 't', 'e', 'r', '-', 'p', 'r', 'i', 'n', 't', ' ', 'c', 'o', 's', 't', 's', ' ', '$', '1', '2', '.', '4', '0', '.', '.', '.']\n",
      "\n",
      "LineTokenizer (1 tokens)\n",
      "['That U.S.A. poster-print costs $12.40...']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "# this is the base class of tokenizers in nltk\n",
    "from nltk.tokenize.api import TokenizerI\n",
    "from nltk.tokenize import NLTKWordTokenizer, MWETokenizer, LineTokenizer\n",
    "\n",
    "# this is just a simple example of how a tokenizer can be implemented\n",
    "class MyWhitespaceTokenizer(TokenizerI):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        return text.split()\n",
    "    \n",
    "sentence = \"That U.S.A. poster-print costs $12.40...\"\n",
    "\n",
    "tokenizers = [\n",
    "    MyWhitespaceTokenizer(),\n",
    "    NLTKWordTokenizer(),\n",
    "    MWETokenizer(),\n",
    "    LineTokenizer()\n",
    " #   LegalitySyllableTokenizer()\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# Leave this function as-is\n",
    "def tokenize(tokenizers: List[TokenizerI], sentence: str) -> None:\n",
    "    for tokenizer in tokenizers:\n",
    "        assert isinstance(tokenizer, TokenizerI)\n",
    "        tokenized = tokenizer.tokenize(sentence)\n",
    "        print(f\"{tokenizer.__class__.__name__} ({len(tokenized)} tokens)\\n{tokenized}\\n\")\n",
    "\n",
    "tokenize(tokenizers, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially i used the wrong coding approch, but it still functioned as expected unsure of what i was doing the results seemed fine, however i was not using classes but rather functions and variables for some reason. Reading more from the source material i found tokenizer classes to import and implemented them into the code instead. <br>\n",
    "The NLTKWord tokenizer splits on both the dollar sign and the tripple dots. Comparing this to the whitespace i think it could improve the understanding of the text, as the combined token would be difficult to replicate. I found the source material to be a little complex and did not quite understand its exact criteria for splitting. <br>\n",
    "The MWETokenizer is made primarly for combining sepsific phrases, given a string argument iy can combine given phrases like 'U.S.A.' into a single token rather than six. With no arguments it splits al characters, even whitespace, into their own tokens. I assume this is best used in combination with common letter combinations or n-grams to utilise its combination effect.<br>\n",
    "I was expecting the line tokenizer to split into sentences, however after reading the documentation i belive it works somewhat differently splitting tokens only when there is a line break and not a sentece end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Language modeling\n",
    "We have now studied the bigger models like BERT and GPT-based language models. A simpler language model, however, can implemented using n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What is an n-gram?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An n-gram refers to the continious sentence of n items. A unigram is a sequence of only a single word, while a bigram contains a sequence of two words, so a n gram is a sequence of n words. It is utilised in problems where the sequence of words are highly relevant. This means it can to a higher degree grasp the context of the words,i assume this is highly helpful when doing word embeddings as the rely heavily on the context of words. Using to large n-grams could lead to over-complex grams and difficulties processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Use NLTK to print out bigrams and trigrams for the given sentence below. Your function should support any number of N.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the sentence in 2-grams\n",
      "('That', 'U.S.A.')\n",
      "('U.S.A.', 'poster-print')\n",
      "('poster-print', 'costs')\n",
      "('costs', '$12.40...')\n",
      "('$12.40...', \"I'd\")\n",
      "(\"I'd\", 'pay')\n",
      "('pay', '$5.00')\n",
      "('$5.00', 'for')\n",
      "('for', 'it.')\n",
      " \n",
      "This is the sentence in 3-grams\n",
      "('That', 'U.S.A.', 'poster-print')\n",
      "('U.S.A.', 'poster-print', 'costs')\n",
      "('poster-print', 'costs', '$12.40...')\n",
      "('costs', '$12.40...', \"I'd\")\n",
      "('$12.40...', \"I'd\", 'pay')\n",
      "(\"I'd\", 'pay', '$5.00')\n",
      "('pay', '$5.00', 'for')\n",
      "('$5.00', 'for', 'it.')\n",
      " \n"
     ]
    }
   ],
   "source": [
    "sentence = \"That U.S.A. poster-print costs $12.40... I'd pay $5.00 for it.\"\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "# ************************************\n",
    "# TODO: your implementation of n-grams\n",
    "# ************************************\n",
    "\n",
    "N = [2, 3]\n",
    "\n",
    "def nGrams(N, sent):\n",
    "    for n in N:\n",
    "        n_grams = ngrams(sent.split(\" \"), n)\n",
    "        print(\"This is the sentence in {}-grams\".format(n))\n",
    "        for grams in n_grams:\n",
    "            print(grams)\n",
    "        print(\" \")\n",
    "\n",
    "nGrams(N, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Based on your intuition for language modeling, how can n-grams be used for word predictions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assume the benefit of using n-grams would be added context to words and using them in combinations with other n-grams. Thus having a better performance when predicting the next word. As mentioned before word embeddings rely on finding similar words based on them being in similar contexts. When prediciting words using context from other words surrounding similar words the predicitons should become easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. NLTK includes the `FreqDist` class, which produces the frequency distribution of words in a sentence. Use it to print out the two most common words in the text below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words with the highest frequency are: \n",
      "[('is', 6), ('that', 5)]\n"
     ]
    }
   ],
   "source": [
    "text = \"That that is is that that is not. Is that it? It is. You sure? Surely it is!\"\n",
    "\n",
    "# There is no text below so i used the text above from question 2.1.\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "freq = FreqDist()\n",
    "for word in word_tokenize(text):\n",
    "    freq[word.lower()] += 1\n",
    "\n",
    "print(\"The words with the highest frequency are: \")\n",
    "print(freq.most_common(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Use your n-gram function from question 2.2 to print out the most common trigram of the text in question 2.4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common  trigram is: \n",
      "[(('that', 'that', 'is'), 2)]\n"
     ]
    }
   ],
   "source": [
    "N = [3]\n",
    "\n",
    "def N_grams(N, text):\n",
    "    for n in N:\n",
    "        n_grams = ngrams(text.lower().split(\" \"), n)\n",
    "        return n_grams\n",
    "\n",
    "grams = N_grams(N, text)\n",
    "\n",
    "freq = FreqDist()\n",
    "for gram in grams:\n",
    "    freq[gram] += 1\n",
    "\n",
    "print(\"The most common  trigram is: \")\n",
    "print(freq.most_common(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. You may have discovered that you would need to implement some form of preprocessing to get the correct answer to the previous tasks. Preprocessing/cleaning/normalization is often necessary for the desired results. If you were to process the text of a news site or blog post, can you think of some preprocessing steps that would be useful?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A method like lowercasing has already been used in a previous task, and could be benefical to remove unnessecary upper casing. It all depends on the task you are going to perform, but removing punctuation, stopwords and special characters can be useful. Opperations like stemming and lemmatization can also be performed as a preprocessing operation.\n",
    "\n",
    "Stemming is the process of reducing words to their stem, an example could be running -> run <br>\n",
    "Lemmatization is similar, but used on words that have a stem or base form unlike its current gramatical form, example better -> good\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Word Representations\n",
    "For more information on word representations, consult the lab description file and course material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Describe the main differences between bag-of-words and one-hot encoding through examples.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words represents the tokens in a frequency chart, where each word is represented by their occurance frequency. One-hot encoding is a binary system where if a word is represented it recives a 1 while unrepresented words are = 0. Using the command FreqDist.tabulate() from the previous exercises we recive a bag of words representation where each word is represented as a frequency number. \n",
    "\n",
    "One hot encoding is used in order to label data within a vector that encompasses the entire \"dictionary\" of words. While Bag-of-words are used to measure their importance in a given document, where you can sort for frequency and remove stopwords to hopfully grasp something about the document contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What are the limitations of the above representations?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words is limited as words with high frequency are often are stop or fill words used mainly for a gramatical purposes. \n",
    "One-hot encoding is limited as the only information you recive is whether the word is present or not, the relative importance of the word to this document or the corpus as a whole is not represented and therefore a limitation using this tecniuqe.\n",
    "\n",
    "Both methods lack context of other words and in a single document it could be difficult to extract the words with relative importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Example of word embedding techniques, such as Word2Vec and GloVe are considered *dense* representations. How do dense word embeddings relate to the *distributional hypothesis*?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributional hypothesis is the linguistic theory that suggest words with similar meaning tend to occur in similar context. This theory is the foundation that dense word embeddings are built upon. The techniques attempt to learn form context and derive classificiations or semantic meaning from the context similarity between words. The method Word2Vec attempts to place all the words in a 2D space and extracting synonyms and correlation between words based on their placement in this space. If the context in this case is the transformed data or rather the 2D space, similar words do indeed occur in similar context, if the model is done correctly. Word embeddings are also used to used to find similar words in the sense of how, man - woman relates to king - queen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
