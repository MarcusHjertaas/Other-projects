{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Given the list of pluralized words below, define your own simple word stemmer function or class,  limited to only simple rules and regex. No libraries! It should strip basic endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fli', 'deni', 'itemiz', 'sensation', 'refer', 'coloni']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plurals = [\n",
    "    \"flies\",\n",
    "    \"denied\",\n",
    "    \"itemization\",\n",
    "    \"sensational\",\n",
    "    \"reference\",\n",
    "    \"colonizer\",\n",
    "]\n",
    "\n",
    "# TODO: implement your own ismple stemmer\n",
    "\n",
    "def stemming(word_list):\n",
    "    suffixes = [\"es\", \"ed\", \"ation\", \"al\", \"ence\", \"zer\"]\n",
    "    stemmed_list, p = [], 0\n",
    "\n",
    "    for word in word_list:\n",
    "        p +=1\n",
    "        for suffix in suffixes:\n",
    "            if word.endswith(suffix):\n",
    "                stem = word[:-len(suffix)]\n",
    "                stemmed_list.append(stem)\n",
    "        \n",
    "        if p > len(stemmed_list):\n",
    "            stemmed_list.append(word)\n",
    "    return stemmed_list\n",
    "\n",
    "stemming(plurals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. After your initial implementation, run it on the following words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['friendly', 'puzzling', 'helpful']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_words = [\n",
    "    \"friendly\",\n",
    "    \"puzzling\",\n",
    "    \"helpful\",\n",
    "]\n",
    "# TODO: run your stemmer on the new words\n",
    "stemming(new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Realizing that fixing future words manually can be problematic, use a desired NLTK stemmer and run it on all the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fli',\n",
       " 'deni',\n",
       " 'item',\n",
       " 'sensat',\n",
       " 'refer',\n",
       " 'colon',\n",
       " 'friendli',\n",
       " 'puzzl',\n",
       " 'help']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "all_words = plurals + new_words\n",
    "\n",
    "# TODO: use an nltk stemming implementation to stem `all_words`\n",
    "\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def stemming_nltk(word_list):\n",
    "    stemmed_list = []\n",
    "    \n",
    "    for word in word_list:\n",
    "        stemmed_list.append(porter.stem(word))\n",
    "\n",
    "    return stemmed_list\n",
    "\n",
    "stemming_nltk(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. There are likely a few words in the outputs above that would cause issues in real-world applications. Pick some examples, and show how they are solved with a lemmatizer. Use either spaCy or nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As my knowledge of english grammatics are limited, the words i picked might not provide a hughe challenge. However puzzeling and puzzl/puzzel do show up in rather different contexts and could provide issues. Helpful -> help is also an issue as they have rather different sentiments. The additional words all get stemmed gramatically wrong and should therefore rather be lemmatized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fly', 'denied', 'colonizer', 'puzzling', 'helpful']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: basic observations on which examples are problematic with stemming + implement lemmatization with spacy/nltk\n",
    "problem_words = [\"flies\", \"denied\", \"colonizer\", \"puzzling\", \"helpful\"]\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def lemmatization_nltk(word_list):\n",
    "    lemmed_list = []\n",
    "    \n",
    "    for word in word_list:\n",
    "        lemmed_list.append(lem.lemmatize(word))\n",
    "\n",
    "    return lemmed_list\n",
    "\n",
    "lemmatization_nltk(problem_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming/Lemmatization - Practical Example\n",
    "Using the news corpus (subset/category of the Brown corpus), perform common text normalization techniques such as stopword filtering and stemming/lemmatization. Compare the top 10 most common **words** before and after these normalization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 6386),\n",
       " (',', 5188),\n",
       " ('.', 4030),\n",
       " ('of', 2861),\n",
       " ('and', 2186),\n",
       " ('to', 2144),\n",
       " ('a', 2130),\n",
       " ('in', 2020),\n",
       " ('for', 969),\n",
       " ('that', 829)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk; nltk.download('brown')  # ensure we have the data\n",
    "from nltk.corpus import brown\n",
    "news = brown.words(categories='news')\n",
    "\n",
    "\n",
    "# TODO: find the top 10 most common words\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "def word_freq(tokenized_corpus, n):\n",
    "    freq = FreqDist()\n",
    "    \n",
    "    for word in tokenized_corpus:\n",
    "        freq[word.lower()] += 1\n",
    "\n",
    "    return freq.most_common(n)\n",
    "\n",
    "word_freq(news, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 406),\n",
       " ('mrs.', 253),\n",
       " ('would', 246),\n",
       " ('year', 244),\n",
       " ('new', 241),\n",
       " ('one', 221),\n",
       " ('state', 213),\n",
       " ('last', 177),\n",
       " ('two', 174),\n",
       " ('mr.', 170)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: find the top 10 most common words after applying text normalization techniques\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(tokenized_corpus):                                      #Added due to lack of experience with the punctuation terms\n",
    "    stop_words = set(stopwords.words(\"english\") + list(string.punctuation) + [\"``\", \"''\", \"--\"])\n",
    "    filtered_corpus = []\n",
    "\n",
    "    for word in tokenized_corpus:\n",
    "        if word.lower() not in stop_words:\n",
    "            filtered_corpus.append(word.lower())\n",
    "    \n",
    "    return filtered_corpus\n",
    "\n",
    "def clean_document(tokenized_corpus):\n",
    "    filtered_corpus = remove_stopwords(tokenized_corpus)\n",
    "    cleaned_corpus = lemmatization_nltk(filtered_corpus)\n",
    "    return cleaned_corpus\n",
    "\n",
    "word_freq(clean_document(news), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "TF-IDF (term frequency-inverse document frequency) is a way to measure the importance of a word in a document.\n",
    "\n",
    "$$\n",
    "\\text{tf-idf}(t, d, D) = \\text{tf}(t, d) \\times \\text{idf}(t, D)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $t$ is the term (word)\n",
    "- $d$ is the document\n",
    "- $D$ is the corpus\n",
    "\n",
    "\n",
    "\n",
    "#### 1. Implement TF-IDF using NLTKs FreqDist (no use of e.g. scikit-learn and other high-level libraries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "##########################################################\n",
    "# Feel free to change everything below.\n",
    "# It is merely a guide to understand the inputs/outputs\n",
    "##########################################################\n",
    "\n",
    "\n",
    "############ TODO ############\n",
    "def tf(document: List[str], term: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the term frequency (TF) of a given term in a document.\n",
    "\n",
    "    Args:\n",
    "        document (List[str]): The document in which to calculate the term frequency.\n",
    "        term (str): The term for which to calculate the term frequency.\n",
    "\n",
    "    Returns:\n",
    "        float: The term frequency of the given term in the document.\n",
    "    \"\"\"\n",
    "    term_freq = FreqDist()\n",
    "    for word in document:\n",
    "        if word.lower() == term:\n",
    "            term_freq[term] += 1\n",
    "    \n",
    "    termfreq = (term_freq[term])/len(document)\n",
    "    return termfreq\n",
    "\n",
    "\n",
    "############ TODO ############\n",
    "def idf(documents: List[List[str]], term: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the inverse document frequency (IDF) of a term in a collection of documents.\n",
    "\n",
    "    Args:\n",
    "        documents (List[List[str]]): A list of documents, where each document is represented as a list of strings.\n",
    "        term (str): The term for which IDF is calculated.\n",
    "\n",
    "    Returns:\n",
    "        float: The IDF value of the term.\n",
    "    \"\"\"\n",
    "    doc_count = 0\n",
    "    for doc in documents:\n",
    "        if term in doc:\n",
    "            doc_count +=1\n",
    "    if doc_count == 0:\n",
    "        return 0\n",
    "    \n",
    "    idf = np.log(len(documents)/(doc_count))\n",
    "    return idf\n",
    "\n",
    "\n",
    "############ TODO ############\n",
    "def tf_idf(all_documents: List[List[str]], document: List[str], term: str) -> float:\n",
    "\n",
    "    term_freq = tf(document, term)\n",
    "    inverse_df = idf(all_documents, term)\n",
    "    tf_idf = term_freq*inverse_df\n",
    "\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. With your TF-IDF function in place, calculate the TF-IDF for the following words in the first document of the news articles found in the Brown corpus: \n",
    "\n",
    "- *the*\n",
    "- *nevertheless*\n",
    "- *highway*\n",
    "- *election*\n",
    "\n",
    "Perform any preprocessing steps you deem necessary. Comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 0.0], ['nevertheless', 0.0010695340199814321], ['highway', 0.008384942647971036], ['election', 0.009251767873746217]]\n"
     ]
    }
   ],
   "source": [
    "fileids = brown.fileids(categories='news')\n",
    "first_doc = list(brown.words(fileids[0]))\n",
    "all_docs = [list(brown.words(fileid)) for fileid in fileids]\n",
    "\n",
    "# TODO: preprocess and calculate tf-idf scores.\n",
    "terms = [\"the\", \"nevertheless\", \"highway\", \"election\"]\n",
    "tf_idf_list = []\n",
    "for term in terms:\n",
    "    p = tf_idf(all_docs, first_doc, term)\n",
    "    tf_idf_list.append([term, p])\n",
    "    \n",
    "print(tf_idf_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. While TF-IDF is primarily used for information retrieval and text mining, reflect on how TF-IDF could be used in a language modeling context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would assume the best use-case for tf-idf in language modelling would be to adjust probabilities in word generators. The tf-idf could not dictate the model entirely as it has no sense of context, but could be a useful tool to find which words are more commonly used. Combining this with the context derived from n gram models i would assume atleast a word predictor og generator would perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. You were previously introduced to word representations. TF-IDF can be considered one. What are some differences between the TF-IDF output and one that is computed once from a vocabulary (e.g. one-hot encoding)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly tf-idf cannot represent words that are not in the doucuments, and have no data of them if they are not represented in the corpus. this could cause problems in certian language models. it also does not have any unique word representation, like one-hot encoding. Each word is represented using an \"adjusted bag-of-words\" approch meaning they lack the possibility of having a unique representation for a word like \"they\". On a positive note, they account for the relative appearance of words, accounting more for stopwords and grammatical fill without needing to remove them from the list of words. As the size of the tf-idf output is created iterativly it should also be a smaller size which could be beneficial if there is a large corpus to be analysed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF - Practical Example\n",
    "You will again be looking at specific words for a document, but this time weighted by their TF-IDF scores. Ideally, the scoring should be able to retrieve representative words for this document in context of its document collection or category.\n",
    "\n",
    "You will do the following:\n",
    "- Select a category from the Reuters (news) corpus\n",
    "- Perform preprocessing\n",
    "- Calculate TF-IDF scores\n",
    "- Find the top 5 words for a subset of documents in your collection (e.g. 5, 10, ..)\n",
    "- Inspect whether these words make sense for a given document, and comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\marcu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk; nltk.download(\"reuters\")\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_tf_idf(corpus, n, N):\n",
    "    for i in range(0, N):\n",
    "        document = corpus[i] \n",
    "        terms_uniq = set(document)\n",
    "        tf_idf_list = []\n",
    "        for term in list(terms_uniq):\n",
    "            score = tf_idf(all_docs, document, term)\n",
    "            tf_idf_list.append([term, score])\n",
    "        tf_idf_list.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(tf_idf_list[:n])\n",
    "        print(document)\n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rubber', 0.10114178009723472], ['exchange', 0.10103927101919714], ['trading', 0.07715828624585008], ['palm', 0.04047353123966693], ['april', 0.03752451660175431]]\n",
      "['indonesian', 'commodity', 'exchange', 'may', 'expand', 'indonesian', 'commodity', 'exchange', 'likely', 'start', 'trading', 'least', 'one', 'new', 'commodity', 'possibly', 'two', 'calendar', '1987', 'exchange', 'chairman', 'paian', 'nainggolan', 'said', 'told', 'reuters', 'telephone', 'interview', 'trading', 'palm', 'oil', 'sawn', 'timber', 'pepper', 'tobacco', 'considered', 'trading', 'either', 'crude', 'palm', 'oil', 'cpo', 'refined', 'palm', 'oil', 'may', 'also', 'introduced', 'said', 'question', 'still', 'considered', 'trade', 'minister', 'rachmat', 'saleh', 'decision', 'go', 'ahead', 'made', 'fledgling', 'exchange', 'currently', 'trade', 'coffee', 'rubber', 'physicals', 'open', 'outcry', 'system', 'four', 'day', 'week', 'several', 'factor', 'make', 'u', 'move', 'cautiously', ',\"', 'nainggolan', 'said', 'want', 'move', 'slowly', 'safely', 'make', 'mistake', 'undermine', 'confidence', 'exchange', '.\"', 'physical', 'rubber', 'trading', 'launched', '1985', 'coffee', 'added', 'january', '1986', 'rubber', 'contract', 'traded', 'fob', 'five', 'month', 'forward', 'robusta', 'coffee', 'grade', 'four', 'five', 'traded', 'prompt', 'delivery', 'five', 'month', 'forward', 'exchange', 'official', 'said', 'trade', 'ministry', 'exchange', 'board', 'considering', 'introduction', 'future', 'trading', 'later', 'rubber', 'one', 'official', 'said', 'feasibility', 'study', 'needed', 'first', 'decision', 'likely', 'indonesia', 'election', 'april', '23', 'trader', 'said', 'trade', 'minister', 'saleh', 'said', 'monday', 'indonesia', 'world', 'second', 'largest', 'producer', 'natural', 'rubber', 'expand', 'rubber', 'marketing', 'effort', 'hoped', 'development', 'exchange', 'would', 'help', 'nainggolan', 'said', 'exchange', 'trying', 'boost', 'overseas', 'interest', 'building', 'contact', 'end', 'user', 'said', 'team', 'already', 'south', 'korea', 'taiwan', 'encourage', 'direct', 'use', 'exchange', 'delegation', 'would', 'also', 'visit', 'europe', 'mexico', 'latin', 'american', 'state', 'encourage', 'participation', 'official', 'say', 'infant', 'exchange', 'made', 'good', 'start', 'although', 'trading', 'coffee', 'disappointing', 'transaction', 'rubber', 'start', 'trading', 'april', '1985', 'december', '1986', 'totalled', '9', '595', 'tonne', 'worth', '6', '9', 'mln', 'dlrs', 'fob', 'plus', '184', '3', 'mln', 'rupiah', 'rubber', 'delivered', 'locally', 'latest', 'exchange', 'report', 'said', 'trading', 'coffee', 'calendar', '1986', 'amounted', '1', '905', 'tonne', '381', 'lot', 'valued', '6', '87', 'billion', 'rupiah', 'total', 'membership', 'exchange', 'nine', 'broker', '44', 'trader']\n",
      "\n",
      "[['colombia', 0.19881219152675372], ['diversify', 0.11987067638945569], ['business', 0.10390737564676848], ['third', 0.08553904236857726], ['aggressively', 0.07710115520516705]]\n",
      "['colombia', 'business', 'asked', 'diversify', 'coffee', 'colombia', 'government', 'trade', 'official', 'urged', 'business', 'community', 'aggressively', 'diversify', 'activity', 'stop', 'relying', 'heavily', 'coffee', 'samuel', 'alberto', 'yohai', 'director', 'foreign', 'trade', 'institute', 'incomex', 'said', 'private', 'businessmen', 'become', 'called', 'mental', 'hostage', 'coffee', 'traditionally', 'colombia', 'major', 'export', 'national', 'planning', 'department', 'forecast', '1987', 'coffee', 'account', 'one', 'third', 'total', 'export', '1', '5', 'billion', 'dlrs', 'oil', 'energy', 'product', 'making', 'another', 'third', 'non', 'traditional', 'export', 'remainder']\n",
      "\n",
      "[['70', 0.11633375645937054], ['80', 0.10901757444906858], ['cent', 0.10901757444906858], ['lb', 0.08316651345351873], ['107', 0.08089301529722445]]\n",
      "['coffee', 'could', 'drop', '70', '80', 'ct', 'cardenas', 'say', 'international', 'coffee', 'price', 'could', 'drop', '70', '80', 'cent', 'lb', 'next', 'october', 'agreement', 'reached', 'support', 'market', 'jorge', 'cardenas', 'manager', 'colombia', 'national', 'coffee', 'grower', 'federation', 'said', 'speaking', 'forum', 'industrialist', 'said', 'one', 'reason', 'market', 'already', 'saturated', 'producer', 'excess', 'production', 'stockpile', '39', 'mln', '60', 'kg', 'bag', '1987', 'today', 'may', 'future', 'new', 'york', 'settled', '107', '90', 'cent', 'lb']\n",
      "\n",
      "[['registration', 0.1612808896368546], ['colombia', 0.1542300637298453], ['arango', 0.1345765618126552], ['bag', 0.10381935968912202], ['registered', 0.09570504873224167]]\n",
      "['colombia', 'coffee', 'registration', 'remain', 'open', 'colombia', 'coffee', 'export', 'registration', 'remain', 'open', 'plan', 'close', 'since', 'new', 'marketing', 'policy', 'mean', 'unlimited', 'amount', 'registered', 'gilberto', 'arango', 'president', 'private', 'exporter', 'association', 'said', 'philosophy', 'new', 'policy', 'close', 'registration', 'nobody', 'far', 'said', 'may', 'would', 'closed', ',\"', 'told', 'reuters', 'march', '13', 'colombia', 'opened', 'registration', 'april', 'may', 'unlimited', 'amount', 'without', 'giving', 'breakdown', 'arango', 'said', 'private', 'exporter', 'registered', '1', '322', '804', 'bag', 'calendar', 'year', 'april', '6', 'roughly', '440', '000', 'bag', 'per', 'month', 'slightly', 'lower', 'average', 'recent', 'year', 'estimated', 'amount', 'bag', 'registered', 'national', 'coffee', 'grower', 'federation', 'meaning', 'total', '900', '000', 'bag', 'registered', 'sold', 'per', 'month', 'colombia', 'change', 'could', 'happen', 'volume', 'would', 'told', 'date', 'registration', 'would', 'june', 'shipment', 'etc', 'arango', 'said']\n",
      "\n",
      "[['nil', 1.0828919368265735], ['rainfall', 0.2099776141757741], ['state', 0.1465159534158787], ['recorded', 0.10498880708788705], ['24', 0.06074537003086927]]\n",
      "['brazilian', 'coffee', 'rainfall', 'following', 'rainfall', 'recorded', 'area', 'past', '24', 'hour', 'parana', 'state', 'umuarama', 'nil', 'paranavai', 'nil', 'londrina', 'nil', 'maringa', 'nil', 'sao', 'paulo', 'state', 'presidente', 'prudente', 'nil', 'votuporanga', 'nil', 'franca', 'nil', 'catanduva', 'nil', 'sao', 'carlos', 'nil', 'sao', 'simao', 'nil', 'mina', 'gerais', 'state', 'guaxupe', 'nil', 'tres', 'pontas', 'nil', 'reuter']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fileids = reuters.fileids(categories='coffee')\n",
    "all_docs = [list(reuters.words(fileid)) for fileid in fileids]\n",
    "corpus = []\n",
    "\n",
    "for doc in all_docs:\n",
    "    corpus.append(clean_document(doc))\n",
    "\n",
    "high_tf_idf(corpus, 5, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general the scores are quite low, perhaps due to some code or mathematical flaw of mine. The first text seems to be more general about trading and exchange as rubber and palm are relativly rare. The third is unique in that is has numbers, and the mention of cent and pounds, perhaps related to business deals or something similar. The mention of colombia is not surprising as they are a major country in coffee production.\n",
    "\n",
    "Appart from that the output seems reasonable, the lack of indonesia mentioned in the first text is a little strange perhaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Briefly describe your understanding of POS tagging and its possible use-cases in context of text generation applications/language modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My understanding is that POS's purpose is to allow the language model to better understand gramatical structures. If you are attempting to predict the next word in a sentence, knowing the previous words gramatical categories would be usefull information. Also if you are generating text, like a chatbot, i would assume there is a need for the model to understand the gramatical structure and how different words fall into certain categories. Tagging words are also beneficial when trying to disambiguate words that are written the same but have different meanings in different contexts. Lastly i would assume it is a helpful addition when doing translations, as the gramatical sturctures of languages are quite different tagging the words should increase the understanding of grammar and by extension the accuracy of the translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train a UnigramTagger (NLTK) using the Brown corpus. \n",
    "Hint: the taggers in nltk require a list of sentences containing tagged words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train a unigram tagger on the brown corpus\n",
    "\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import brown\n",
    "\n",
    "tagged_sents = brown.tagged_sents()\n",
    "\n",
    "unigram_tagger = UnigramTagger(tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Use this tagger to tag the text given below. Print out the POS tags for all variants of \"justify\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VB', None, 'NNS', 'VBG', 'VB', None]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Imagine a situation where you have to explain why you did something – that's when you justify your actions. So, let's say you made a decision; you, as the justifier, need to give good reasons (justifications) for your choice. You might use justifying words to make your point clear and reasonable. Justifying can be a bit like saying, \"Here's why I did what I did.\" When you justify things, you're basically providing the why behind your actions. So, being a good justifier involves carefully explaining, giving reasons, and making sure others understand your choices\"\"\"\n",
    "\n",
    "# TODO: use your trained tagger\n",
    "\n",
    "text_words = nltk.word_tokenize(text)\n",
    "\n",
    "tags = unigram_tagger.tag(text_words)\n",
    "just_tags = [tag for word, tag in tags if word.startswith(\"justi\")]\n",
    "print(just_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Your results may be disappointing. Repeat the same task as above using both the default NLTK pos-tagger and with spaCy. Compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VBP', 'NN', 'NNS', 'VBG', 'VBP', 'NN']\n"
     ]
    }
   ],
   "source": [
    "# TODO: use the default NLTK tagger\n",
    "\n",
    "tags = nltk.pos_tag(text_words)\n",
    "just_tags = [tag for word, tag in tags if word.startswith(\"justi\")]\n",
    "print(just_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VBP', 'NN', 'NNS', 'VBG', 'VBP', 'NN']\n"
     ]
    }
   ],
   "source": [
    "# TODO: use spacy to fetch pos tags from the document\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "just_tags_spacy = [token.tag_ for token in doc if token.text.startswith(\"justi\")]\n",
    "print(just_tags_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They produce the exact same results, both are an improvement on the unigram tagger. Perhaps the methods i used were not the ones intended or they were simply both meant to conclude with the same answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Finally, explore more features of the what the spaCy *document* includes related to topics covered in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token           Lemma           POS Tag   \n",
      "Imagine         imagine         VERB      \n",
      "a               a               DET       \n",
      "situation       situation       NOUN      \n",
      "where           where           SCONJ     \n",
      "you             you             PRON      \n",
      "have            have            VERB      \n",
      "to              to              PART      \n",
      "explain         explain         VERB      \n",
      "why             why             SCONJ     \n",
      "you             you             PRON      \n",
      "did             do              VERB      \n",
      "something       something       PRON      \n",
      "–               –               PUNCT     \n",
      "that            that            PRON      \n",
      "'s              be              AUX       \n",
      "when            when            SCONJ     \n",
      "you             you             PRON      \n",
      "justify         justify         VERB      \n",
      "your            your            PRON      \n",
      "a               a               PRON      \n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text[:100])\n",
    "\n",
    "print(f\"{'Token':<15} {'Lemma':<15} {'POS Tag':<10}\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<15} {token.lemma_:<15} {token.pos_:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure this is what you intended but i utilised more of the spacy library to lemmatize and tagg part of the text from earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
